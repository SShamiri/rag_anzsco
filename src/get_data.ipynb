{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e87543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c001efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_links_from_sitemap(sitemap_url):\n",
    "    \"\"\"\n",
    "    Downloads all links from a sitemap URL and returns them in a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sitemap_url (str): The URL of the sitemap XML file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with a single 'url' column containing the links.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching sitemap from: {sitemap_url}\")\n",
    "    try:\n",
    "        # 1. Fetch and parse the sitemap to get all page URLs\n",
    "        sitemap_response = requests.get(sitemap_url)\n",
    "        sitemap_response.raise_for_status()  # Ensure the request was successful\n",
    "        sitemap_soup = BeautifulSoup(sitemap_response.content, 'xml')\n",
    "        \n",
    "        # The URLs are within <loc> tags in the XML\n",
    "        urls = [loc.text for loc in sitemap_soup.find_all('loc')]\n",
    "        print(f\"Found {len(urls)} URLs in the sitemap.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch sitemap: {e}\")\n",
    "        return pd.DataFrame({'url': []}) # Return an empty DataFrame on failure\n",
    "\n",
    "    # 2. Create a pandas DataFrame directly from the list of URLs\n",
    "    df = pd.DataFrame({'url': urls})\n",
    "    print(\"\\nLink download complete!\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc61a33",
   "metadata": {},
   "source": [
    "### Dowmload page 5 & 6\n",
    "- select filter for anzsco and year 2021 & 2022\n",
    "- split the url by anzsco 1dig, 2dig, 4dig $ 6dig\n",
    "- remove the duplicates consider pirortising year 2022\n",
    "- save the url's into df for docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c96cf355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sitemap from: https://www.abs.gov.au/sitemap.xml?page=6\n",
      "Found 2000 URLs in the sitemap.\n",
      "\n",
      "Link download complete!\n",
      "\n",
      "--- DataFrame Head ---\n",
      "                                                 url\n",
      "0  https://www.abs.gov.au/statistics/classificati...\n",
      "1  https://www.abs.gov.au/statistics/classificati...\n",
      "2  https://www.abs.gov.au/statistics/classificati...\n",
      "3  https://www.abs.gov.au/statistics/classificati...\n",
      "4  https://www.abs.gov.au/statistics/classificati...\n"
     ]
    }
   ],
   "source": [
    "# The specific sitemap URL from your request\n",
    "SITEMAP_PAGE_6 = \"https://www.abs.gov.au/sitemap.xml?page=6\"\n",
    "\n",
    "# Run the function\n",
    "links_df = download_links_from_sitemap(SITEMAP_PAGE_6)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(\"\\n--- DataFrame Head ---\")\n",
    "print(links_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "373d5e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anzsco_df = links_df[links_df['url'].str.contains('anzsco', case=False, na=False)]\n",
    "len(anzsco_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "787c9b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anzsco_22_df = anzsco_df[anzsco_df['url'].str.contains('2022', case=False, na=False)]\n",
    "len(anzsco_22_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b30ec3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "anzsco_22_df.to_csv('../data/anzsco_22_links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7d1dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.abs.gov.au/statistics/classifications/anzsco-australian-and-new-zealand-standard-classification-occupations/2022/browse-classification/5/59/599/5994\n"
     ]
    }
   ],
   "source": [
    "print(anzsco_22_df['url'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d916e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_abs_sitemap(sitemap_url):\n",
    "    \"\"\"\n",
    "    Scrapes a specific sitemap URL from the Australian Bureau of Statistics,\n",
    "    extracts the content from each page listed, and returns a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sitemap_url (str): The URL of the sitemap XML file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with 'url' and 'content' columns.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching sitemap from: {sitemap_url}\")\n",
    "    try:\n",
    "        # 1. Fetch and parse the sitemap to get all page URLs\n",
    "        sitemap_response = requests.get(sitemap_url)\n",
    "        sitemap_response.raise_for_status()  # Ensure the request was successful\n",
    "        sitemap_soup = BeautifulSoup(sitemap_response.content, 'xml')\n",
    "        \n",
    "        # The URLs are within <loc> tags in the XML\n",
    "        urls = [loc.text for loc in sitemap_soup.find_all('loc')]\n",
    "        print(f\"Found {len(urls)} URLs in the sitemap.\")\n",
    "\n",
    "        # 2. Scrape the content from each URL\n",
    "        data = []\n",
    "        for i, url in enumerate(urls):\n",
    "            print(f\"Scraping ({i+1}/{len(urls)}): {url}\")\n",
    "            try:\n",
    "                page_response = requests.get(url, timeout=10)\n",
    "                page_response.raise_for_status()\n",
    "                \n",
    "                page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "                \n",
    "                # The main document content on ABS pages is typically within\n",
    "                # an <article> tag with the class 'main-content'.\n",
    "                content_area = page_soup.find('article', class_='main-content')\n",
    "                \n",
    "                if content_area:\n",
    "                    # Extract text, using a space as a separator and stripping whitespace\n",
    "                    content_text = content_area.get_text(separator=' ', strip=True)\n",
    "                else:\n",
    "                    content_text = \"Main content area not found.\"\n",
    "                    \n",
    "                data.append({'url': url, 'content': content_text})\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  -> Failed to retrieve {url}: {e}\")\n",
    "                data.append({'url': url, 'content': f\"Error: Could not retrieve page.\"})\n",
    "            \n",
    "            # Be polite and avoid overwhelming the server\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch sitemap: {e}\")\n",
    "        return pd.DataFrame() # Return an empty DataFrame on failure\n",
    "\n",
    "    # 3. Create a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nScraping complete!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ea03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The specific sitemap URL from your request\n",
    "SITEMAP_PAGE_5 = \"https://www.abs.gov.au/sitemap.xml?page=5\"\n",
    "\n",
    "# Run the scraper\n",
    "abs_df = scrape_abs_sitemap(SITEMAP_PAGE_5)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(\"\\n--- DataFrame Head ---\")\n",
    "print(abs_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
